{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea8198c",
   "metadata": {},
   "source": [
    "# Install, Import, and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q torch==2.7.0 ms-swift==3.7.2 transformers==4.51.3 trl pydantic peft torchvision tqdm ipywidgets torchmetrics bitsandbytes accelerate protobuf pandas decord tokenizers sentencepiece pyarrow pydantic_core markdown2[all] numpy scikit-learn requests httpx uvicorn fastapi einops einops-exts timm tiktoken transformers_stream_generator scipy pandas torchaudio xformers pillow deepspeed pysubs2 moviepy==1.0.3 gradio\n",
    "# ! pip install -q https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ba4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "import binascii\n",
    "import re\n",
    "import smtplib\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6965e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)\n",
    "\n",
    "def set_logging(file_path):\n",
    "    ! rm {file_path}\n",
    "    nblog = open(file_path, \"a+\")\n",
    "    sys.stdout.echo = nblog\n",
    "    sys.stderr.echo = nblog\n",
    "    get_ipython().log.handlers[0].stream = nblog\n",
    "    get_ipython().log.setLevel(logging.INFO)\n",
    "    %autosave 5\n",
    "set_logging(\"logs/whole.log\")\n",
    "    \n",
    "def send_email(subject, text):\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 25)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login(address := 'chio4696@gmail.com', 'xnhunyoqaqvflpgw')\n",
    "    smtpObj.sendmail(address, address, f\"Subject: {subject}\\n{text}\\n\")\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848628ee",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9722a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row_data):\n",
    "    index, row, save_dir = row_data\n",
    "    if row['input_type'] != 'image':\n",
    "        return index, row['input']\n",
    "    try:\n",
    "        image_data = row['input']\n",
    "        filename = hashlib.sha256(str(image_data).encode()).hexdigest() + '.jpg'\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "        image_bytes = None\n",
    "        if isinstance(image_data, str) and image_data.startswith(('http', 'https')):\n",
    "            response = requests.get(image_data, stream=True, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            image_bytes = response.raw\n",
    "        elif isinstance(image_data, bytes):\n",
    "            image_bytes = BytesIO(image_data)\n",
    "        elif isinstance(image_data, str):\n",
    "            try:\n",
    "                decoded_data = base64.b64decode(image_data)\n",
    "                image_bytes = BytesIO(decoded_data)\n",
    "            except (binascii.Error, ValueError):\n",
    "                return index, None\n",
    "        if image_bytes:\n",
    "            with Image.open(image_bytes) as img:\n",
    "                img.convert('RGB').save(save_path, 'jpeg')\n",
    "            return index, save_path\n",
    "        else:\n",
    "            return index, None\n",
    "    except Exception as e:\n",
    "        return index, None\n",
    "\n",
    "def preprocess_with_executor(parquet_path, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Created directory: {save_dir}\")\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"Successfully loaded {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return\n",
    "    tasks = [(index, row, save_dir) for index, row in df.iterrows()]\n",
    "    new_input_paths = [None] * len(df)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_row, task): task for task in tasks}\n",
    "        for future in tqdm(as_completed(futures), total=len(tasks), desc=f\"Processing {os.path.basename(parquet_path)}\"):\n",
    "            index, new_path = future.result()\n",
    "            if index is not None and index < len(new_input_paths):\n",
    "                if new_path is None and df.iloc[index]['input_type'] != 'image':\n",
    "                    new_input_paths[index] = df.iloc[index]['input']\n",
    "                else:\n",
    "                    new_input_paths[index] = new_path\n",
    "    df['input'] = new_input_paths\n",
    "    output_filename = os.path.splitext(os.path.basename(parquet_path))[0] + '_path_converted.parquet'\n",
    "    output_path = os.path.join(os.path.dirname(parquet_path), output_filename)\n",
    "    try:\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        print(f\"Successfully saved processed data to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed parquet file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "384ca469",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet_file = '../data/raw/deep_chal_multitask_dataset.parquet'\n",
    "train_image_save_directory = '../data/image/train_images'\n",
    "preprocess_with_executor(train_parquet_file, train_image_save_directory)\n",
    "\n",
    "test_parquet_file = '../data/raw/deep_chal_multitask_dataset_test.parquet'\n",
    "test_image_save_directory = '../data/image/test_images'\n",
    "preprocess_with_executor(test_parquet_file, test_image_save_directory)\n",
    "\n",
    "train = pd.read_parquet(\"../data/raw/deep_chal_multitask_dataset_path_converted.parquet\")\n",
    "train = train[train['input'].notna()]\n",
    "train.to_parquet(\"../data/converted/deep_chal_multitask_dataset_path_converted.parquet\", index=False)\n",
    "\n",
    "test = pd.read_parquet(\"../data/raw/deep_chal_multitask_dataset_test_path_converted.parquet\")\n",
    "test.to_parquet(\"../data/converted/deep_chal_multitask_dataset_test_path_converted.parquet\", index=False)\n",
    "\n",
    "send_email(\"Preprocessing Done\", \"It is.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bad3b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penultimate Train DF:  46638\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_path_converted.parquet\")\n",
    "\n",
    "text_qa_df = train[train[\"task\"] == \"text_qa\"].copy()\n",
    "all_qa_pairs = []\n",
    "for index, row in text_qa_df.iterrows():\n",
    "    context = row['input']\n",
    "    answers_dict = ast.literal_eval(row['output'])\n",
    "    questions_list = ast.literal_eval(row['question'])\n",
    "    answer_texts = answers_dict.get('input_text', [])\n",
    "    for question, answer in zip(questions_list, answer_texts):\n",
    "        all_qa_pairs.append({'input': context, 'question': question, 'output': answer})\n",
    "processed_qa_df = pd.DataFrame(all_qa_pairs)\n",
    "pronouns_to_remove = ['it', 'this', 'that', 'these', 'those', 'he', 'she', 'they', 'them', 'him', 'her', 'his', 'its', 'their', 'theirs']\n",
    "pattern = r'\\b(' + '|'.join(pronouns_to_remove) + r')\\b'\n",
    "mask_to_remove = processed_qa_df['question'].str.contains(pattern, case=False, regex=True)\n",
    "final_filtered_df = processed_qa_df[~mask_to_remove]\n",
    "allowed_start_words = ('What', 'How', 'Why', 'When', 'Where', 'Who', 'Is', 'Are', 'Am', 'Were', 'Was', 'Which', \"List\", \"Did\", \"Before\", \"After\", \"Name\", \"Do\", 'Can', 'Could', 'From', 'About', 'According', 'At', 'Can', 'Could', 'Define', 'Describe', 'Does', 'During', 'For', 'From', 'Had', 'Has', 'Have', 'In', 'On', 'Under', 'Whom', 'Whose', 'Will')\n",
    "mask_starts_with = final_filtered_df['question'].str.startswith(allowed_start_words)\n",
    "mask_long_enough = final_filtered_df['question'].str.split().str.len() >= 6\n",
    "final_mask = mask_starts_with & mask_long_enough\n",
    "final_filtered_df = final_filtered_df[final_mask]\n",
    "other_tasks_df = train[train['task'] != 'text_qa'].copy()\n",
    "final_filtered_df[\"input_type\"] = \"text\"\n",
    "final_filtered_df[\"task\"] = \"text_qa\"\n",
    "final_train_df = pd.concat([other_tasks_df, final_filtered_df], ignore_index=True)\n",
    "train = final_train_df.reindex()\n",
    "print(\"Penultimate Train DF: \", len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64aa5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original task distribution:\n",
      "task\n",
      "text_qa           15860\n",
      "summarization     10000\n",
      "vqa               10000\n",
      "math_reasoning     7473\n",
      "captioning         3305\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Task distribution after undersampling 'text_qa':\n",
      "task\n",
      "text_qa           11229\n",
      "summarization     10000\n",
      "vqa               10000\n",
      "math_reasoning     7473\n",
      "captioning         3305\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Dataset Split ---\n",
      "Total samples: 42007\n",
      "SFT samples (90%): 37806\n",
      "\n",
      "--- Task Distribution in SFT set ---\n",
      "task\n",
      "text_qa           0.267312\n",
      "vqa               0.238057\n",
      "summarization     0.238057\n",
      "math_reasoning    0.177908\n",
      "captioning        0.078665\n",
      "Name: proportion, dtype: float64\n",
      "--------------------\n",
      "Saved ../data/dataset/sft_data_stratified.jsonl\n"
     ]
    }
   ],
   "source": [
    "IMAGE_BASE_PATH = '../data/image/train_images/'\n",
    "SFT_FILE = '../data/dataset/sft_data_stratified.jsonl'\n",
    "# VAL_FILE = '../data/dataset/val_data_stratified.jsonl'\n",
    "SPLIT_RATIO = 0.1\n",
    "\n",
    "df = train.copy()\n",
    "df['question'] = df['question'].fillna('')\n",
    "df['input'] = df['input'].fillna('')\n",
    "print(\"Original task distribution:\")\n",
    "print(df['task'].value_counts())\n",
    "\n",
    "df_text_qa = df[df['task'] == 'text_qa']\n",
    "df_other_tasks = df[df['task'] != 'text_qa']\n",
    "df_text_qa_undersampled = df_text_qa.groupby('input', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 3), random_state=42))\n",
    "df = pd.concat([df_other_tasks, df_text_qa_undersampled])\n",
    "print(\"\\nTask distribution after undersampling 'text_qa':\")\n",
    "print(df['task'].value_counts())\n",
    "\n",
    "sft_df, val_df = train_test_split(df, test_size=SPLIT_RATIO, random_state=42, stratify=df['task'])\n",
    "\n",
    "print(\"\\n--- Dataset Split ---\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"SFT samples (90%): {len(sft_df)}\")\n",
    "\n",
    "print(\"\\n--- Task Distribution in SFT set ---\")\n",
    "print(sft_df['task'].value_counts(normalize=True))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "with open(SFT_FILE, 'w') as f:\n",
    "    for _, row in sft_df.iterrows():\n",
    "        task = row['task']\n",
    "        question = row['question']\n",
    "        output = row['output']\n",
    "        record = {}\n",
    "        if task == 'captioning':\n",
    "            query = '<image> Generate a single, detailed, and objective descriptive paragraph for the given image. Each description must begin with the phrase \"The image is...\" or \"The image shows...\", followed by a structured analysis that moves from the main subject to its details, and then to the background elements. You must use positional language, such as \"on the left\" or \"at the top of the cover\" to clearly orient the reader. If any text is visible in the image, transcribe it exactly and describe its visual characteristics like color and style. Conclude the entire description with a sentence that summarizes the overall atmosphere of the image, using a phrase like \"The overall mood of the image is...\". Throughout the paragraph, maintain a strictly factual, declarative tone with specific, descriptive vocabulary, avoiding any personal opinions or interpretations.'\n",
    "            image_path = os.path.join(IMAGE_BASE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"output\": output, \"images\": [image_path]}\n",
    "        elif task == 'vqa':\n",
    "            query = f'<image> Given a document image and a question, extract the precise answer. Your response must be only the literal text found in the image, with no extra words or explanation.\\n\\nQuestion: {question}'\n",
    "            image_path = os.path.join(IMAGE_BASE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"output\": output, \"images\": [image_path]}\n",
    "        elif task == 'summarization':\n",
    "            prompt = f\"Generate a summary of the following legislative text. Start with the bill's official title, then state its primary purpose and key provisions. Use formal, objective language and focus on the actions the bill takes, such as what it amends, requires, prohibits, or establishes.\\n\\nText: {row['input']}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        elif task == 'text_qa':\n",
    "            prompt = f\"Given a context and a question, extract the most concise, direct answer from the text. Your answer should be a short phrase, not a complete sentence.\\n\\nContext: {row['input']}\\n\\nQuestion: {question}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        elif task == 'math_reasoning':\n",
    "            prompt = f\"Given a math word problem, solve the question by generating a step-by-step reasoning process. After detailing all the steps in your reasoning, you must conclude your response by placing the final numerical answer on its own separate line, prefixed with #### .\\n\\nQuestion: {row['input']}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "print(f\"Saved {SFT_FILE}\")\n",
    "\n",
    "# NO VALIDATION FOR FASTER REPRODUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7ffa417",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptioning\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<image> Generate a single, detailed, and objective descriptive paragraph for the given image. Each description must begin with the phrase \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image is...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image shows...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, followed by a structured analysis that moves from the main subject to its details, and then to the background elements. You must use positional language, such as \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon the left\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat the top of the cover\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to clearly orient the reader. If any text is visible in the image, transcribe it exactly and describe its visual characteristics like color and style. Conclude the entire description with a sentence that summarizes the overall atmosphere of the image, using a phrase like \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe overall mood of the image is...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Throughout the paragraph, maintain a strictly factual, declarative tone with specific, descriptive vocabulary, avoiding any personal opinions or interpretations.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_BASE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     record \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [image_path]}\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqa\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/posixpath.py:90\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     88\u001b[0m             path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sep \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mBytesWarning\u001b[39;00m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mgenericpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_arg_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[0;34m(funcname, *args)\u001b[0m\n\u001b[1;32m    150\u001b[0m         hasbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() argument must be str, bytes, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    153\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike object, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasstr \u001b[38;5;129;01mand\u001b[39;00m hasbytes:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt mix strings and bytes in path components\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "test = pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_test_path_converted.parquet\")\n",
    "IMAGE_BASE_PATH = '../data/image/test_images/'\n",
    "TEST_FILE = '../data/dataset/test_data.jsonl'\n",
    "test_df = test.copy()\n",
    "\n",
    "with open(TEST_FILE, 'w') as f:\n",
    "    for _, row in test_df.iterrows():\n",
    "        task = row['task']\n",
    "        question = row['question']\n",
    "        record = {}\n",
    "        if task == 'captioning':\n",
    "            query = '<image> Generate a single, detailed, and objective descriptive paragraph for the given image. Each description must begin with the phrase \"The image is...\" or \"The image shows...\", followed by a structured analysis that moves from the main subject to its details, and then to the background elements. You must use positional language, such as \"on the left\" or \"at the top of the cover\" to clearly orient the reader. If any text is visible in the image, transcribe it exactly and describe its visual characteristics like color and style. Conclude the entire description with a sentence that summarizes the overall atmosphere of the image, using a phrase like \"The overall mood of the image is...\". Throughout the paragraph, maintain a strictly factual, declarative tone with specific, descriptive vocabulary, avoiding any personal opinions or interpretations.'\n",
    "            image_path = os.path.join(IMAGE_BASE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"images\": [image_path]}\n",
    "        elif task == 'vqa':\n",
    "            query = f'<image> Given a document image and a question, extract the precise answer. Your response must be only the literal text found in the image, with no extra words or explanation.\\n\\nQuestion: {question}'\n",
    "            image_path = os.path.join(IMAGE_BASE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"images\": [image_path]}\n",
    "        elif task == 'summarization':\n",
    "            prompt = f\"Generate a summary of the following legislative text. Start with the bill's official title, then state its primary purpose and key provisions. Use formal, objective language and focus on the actions the bill takes, such as what it amends, requires, prohibits, or establishes.\\n\\nText: {row['input']}\"\n",
    "            record = {\"input\": prompt}\n",
    "        elif task == 'text_qa':\n",
    "            prompt = f\"Given a context and a question, extract the most concise, direct answer from the text. Your answer should be a short phrase, not a complete sentence.\\n\\nContext: {row['input']}\\n\\nQuestion: {question}\"\n",
    "            record = {\"input\": prompt}\n",
    "        elif task == 'math_reasoning':\n",
    "            prompt = f\"Given a math word problem, solve the question by generating a step-by-step reasoning process. After detailing all the steps in your reasoning, you must conclude your response by placing the final numerical answer on its own separate line, prefixed with #### .\\n\\nQuestion: {row['input']}\"\n",
    "            record = {\"input\": prompt}\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "print(f\"Saved {TEST_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_path_converted.parquet\").groupby(\"task\").head(1))\n",
    "display(pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_test_path_converted.parquet\").groupby(\"task\").head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2af214",
   "metadata": {},
   "source": [
    "# SFT Experimental Runs w/ CE Loss vs. Dynamic Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAY SET # OF DEVICE TO 0, 1 AND RUN EACH ON TERMINAL TO UTILIZE MULTIPLE GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model AIDC-AI/Ovis2.5-9B \\\n",
    "    --dataset '../data/dataset/sft_data_stratified.jsonl' \\\n",
    "    --dataloader_num_workers 16 \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir 'output_ce_loss' \\\n",
    "    --use_hf true \\\n",
    "    --train_type lora \\\n",
    "    --lora_rank 64 \\\n",
    "    --lora_alpha 128 \\\n",
    "    --init_weights pissa \\\n",
    "    --use_rslora true \\\n",
    "    --target_modules all-linear \\\n",
    "    --freeze_vit true \\\n",
    "    --freeze_llm false \\\n",
    "    --freeze_aligner true \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --max_length 8192 \\\n",
    "    --loss_scale ignore_empty_think \\\n",
    "    --attn_impl flash_attn \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --padding_free true \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db889eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model AIDC-AI/Ovis2.5-9B \\\n",
    "    --dataset '../data/dataset/sft_data_stratified.jsonl' \\\n",
    "    --dataloader_num_workers 16 \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir 'output_dynamic_loss' \\\n",
    "    --use_hf true \\\n",
    "    --train_type lora \\\n",
    "    --lora_rank 64 \\\n",
    "    --lora_alpha 128 \\\n",
    "    --init_weights pissa \\\n",
    "    --use_rslora true \\\n",
    "    --target_modules all-linear \\\n",
    "    --freeze_vit true \\\n",
    "    --freeze_llm false \\\n",
    "    --freeze_aligner true \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --max_length 8192 \\\n",
    "    --enable_dft_loss true \\\n",
    "    --loss_scale ignore_empty_think \\\n",
    "    --attn_impl flash_attn \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --padding_free true \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"SFT Done\", \"WAKE UP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de063d3d",
   "metadata": {},
   "source": [
    "# Inference, Read Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c470990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# **** CHANGE THE ADAPTERS PATH INTO EACH RUN'S OUTPUT/SOME_VERSION/\"CHECKPOINT-{}\" PATH TO LOAD PROPERLY ****\n",
    "# **** AFTER SUBMISSION FILE CREATION, SUBMIT AND USE SUBMISSION WITH HIGHER SCORING ON LB ****\n",
    "# **** PLAY WITH TEMPERATURE, USED TEMP=[0 (for run 1 and 2), 0.3 (for run 1)] ****\n",
    "# **** CURRENT ADAPTER PATH IS SET TO THE ADAPTER I USED IN THE COMPETITION ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift infer \\\n",
    "    --adapters 'output/v0-20250828-230749/checkpoint-2733/' \\\n",
    "    --infer_backend pt \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 4096 \\\n",
    "    --val_dataset '../data/dataset/test_data.jsonl' \\\n",
    "    --use_hf true \\\n",
    "    --max_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** BE SURE TO CHANGE THE PREDICTION / OUTPUT PATH FOR SECOND RUN SUBMISSION FILE CREATION ****\n",
    "# **** THE PREDICTION PATH IS AT THE END OF LAST CELL OUTPUT ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"../data/raw/\")\n",
    "pred = pd.read_json(\"output/v0-20250828-230749/checkpoint-2733/infer_result/20250830-115559.jsonl\", lines=True)\n",
    "test[\"output\"] = pred[\"response\"]\n",
    "test.index.name = \"id\"\n",
    "test[\"output\"].to_csv(\"../prediction/submission_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"Inference Done\", \"Check LB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52173966",
   "metadata": {},
   "source": [
    "# Simple ENV out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3163c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 freeze > ../requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
