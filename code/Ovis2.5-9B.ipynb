{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea8198c",
   "metadata": {},
   "source": [
    "# Install, Import, and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q torch==2.7.0 ms-swift==3.7.2 transformers==4.51.3 trl pydantic peft torchvision tqdm ipywidgets torchmetrics bitsandbytes accelerate protobuf pandas decord tokenizers sentencepiece pyarrow pydantic_core markdown2[all] numpy scikit-learn requests httpx uvicorn fastapi einops einops-exts timm tiktoken transformers_stream_generator scipy pandas torchaudio xformers pillow deepspeed pysubs2 moviepy==1.0.3 gradio\n",
    "# ! pip install -q https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ba4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "import binascii\n",
    "import re\n",
    "import smtplib\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6965e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(5000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)\n",
    "\n",
    "def set_logging(file_path):\n",
    "    ! rm {file_path}\n",
    "    nblog = open(file_path, \"a+\")\n",
    "    sys.stdout.echo = nblog\n",
    "    sys.stderr.echo = nblog\n",
    "    get_ipython().log.handlers[0].stream = nblog\n",
    "    get_ipython().log.setLevel(logging.INFO)\n",
    "    %autosave 5\n",
    "set_logging(\"logs/whole.log\")\n",
    "    \n",
    "def send_email(subject, text):\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 25)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login(address := 'chio4696@gmail.com', 'xnhunyoqaqvflpgw')\n",
    "    smtpObj.sendmail(address, address, f\"Subject: {subject}\\n{text}\\n\")\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848628ee",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db4e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_row(row_data):\n",
    "#     index, row, save_dir = row_data\n",
    "#     if row.get('input_type') != 'image':\n",
    "#         return index, row['input'], None\n",
    "#     try:\n",
    "#         image_data = row['input']\n",
    "#         filename = hashlib.sha256(str(image_data).encode()).hexdigest() + '.jpg'\n",
    "#         save_path = os.path.join(save_dir, filename)\n",
    "#         if os.path.exists(save_path):\n",
    "#             return index, save_path, None\n",
    "#         image_bytes = None\n",
    "#         if isinstance(image_data, str) and image_data.startswith(('http', 'https')):\n",
    "#             headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "#             response = requests.get(image_data, stream=True, timeout=15, headers=headers)\n",
    "#             response.raise_for_status()\n",
    "#             image_bytes = BytesIO(response.content)\n",
    "#         elif isinstance(image_data, bytes):\n",
    "#             image_bytes = BytesIO(image_data)\n",
    "#         elif isinstance(image_data, str):\n",
    "#             try:\n",
    "#                 decoded_data = base64.b64decode(image_data)\n",
    "#                 image_bytes = BytesIO(decoded_data)\n",
    "#             except (binascii.Error, ValueError) as e:\n",
    "#                 return index, None, f\"Base64 Decode Error: {e}\"\n",
    "#         if image_bytes:\n",
    "#             img = Image.open(image_bytes)\n",
    "#             img.verify()\n",
    "#             image_bytes.seek(0)\n",
    "#             with Image.open(image_bytes) as img_to_save:\n",
    "#                 img_to_save.convert('RGB').save(save_path, 'jpeg')\n",
    "#             return index, save_path, None\n",
    "#         else:\n",
    "#             return index, None, \"Image data could not be processed\"\n",
    "#     except requests.exceptions.HTTPError as e:\n",
    "#         return index, None, f\"HTTP Error: {e.response.status_code} {e.response.reason}\"\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         return index, None, f\"Request Error: {type(e).__name__}\"\n",
    "#     except UnidentifiedImageError:\n",
    "#         return index, None, \"PIL Error: corrupted or invalid format\"\n",
    "#     except Exception as e:\n",
    "#         return index, None, f\"General Error: {e}\"\n",
    "\n",
    "# def preprocess_with_executor(parquet_path, save_dir, max_retries=5):\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     try:\n",
    "#         df = pd.read_parquet(parquet_path)\n",
    "#     except Exception as e:\n",
    "#         return\n",
    "#     new_input_paths = df['input'].copy()\n",
    "#     image_mask = df['input_type'] == 'image'\n",
    "#     tasks_to_process = [(index, row, save_dir) for index, row in df[image_mask].iterrows()]\n",
    "#     for i in range(max_retries):\n",
    "#         if not tasks_to_process:\n",
    "#             print(\"All tasks processed.\")\n",
    "#             break\n",
    "#         print(f\"\\n--- Retry Loop {i + 1}/{max_retries} ---\")\n",
    "#         print(f\"Processing {len(tasks_to_process)} tasks...\")\n",
    "#         failed_tasks = []\n",
    "#         error_counts = Counter()\n",
    "#         with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "#             future_to_task = {executor.submit(process_row, task): task for task in tasks_to_process}\n",
    "#             progress_bar = tqdm(as_completed(future_to_task), total=len(tasks_to_process), desc=f\"Loop {i+1}\")\n",
    "#             for future in progress_bar:\n",
    "#                 original_task = future_to_task[future]\n",
    "#                 index, new_path, error = future.result()\n",
    "#                 if new_path:\n",
    "#                     new_input_paths.loc[index] = new_path\n",
    "#                 else:\n",
    "#                     failed_tasks.append(original_task)\n",
    "#                     if error:\n",
    "#                         error_counts[error] += 1\n",
    "#         print(f\"Loop {i + 1} complete. {len(failed_tasks)} tasks failed and will be retried.\")\n",
    "#         if error_counts:\n",
    "#             print(\"Top 5 errors in this loop:\")\n",
    "#             for error_msg, count in error_counts.most_common(5):\n",
    "#                 print(f\"  - [{count} times] {error_msg}\")\n",
    "#         tasks_to_process = failed_tasks\n",
    "#     if tasks_to_process:\n",
    "#         print(f\"\\nWarning: After {max_retries} retries, {len(tasks_to_process)} tasks still failed.\")\n",
    "#     df['input'] = new_input_paths\n",
    "#     output_filename = os.path.splitext(os.path.basename(parquet_path))[0] + '_path_converted.parquet'\n",
    "#     output_path = os.path.join(os.path.dirname(parquet_path), output_filename)\n",
    "#     df.to_parquet(output_path, index=False)\n",
    "#     print(f\"Saved processed data to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e616f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_with_executor('../data/raw/deep_chal_multitask_dataset.parquet', '../data/image/train_images')\n",
    "# train = pd.read_parquet('../data/raw/deep_chal_multitask_dataset_path_converted.parquet')\n",
    "# train = train[train['input'].notna()]\n",
    "# train.to_parquet(\"../data/converted/deep_chal_multitask_dataset_path_converted.parquet\", index=False)\n",
    "\n",
    "# preprocess_with_executor('../data/raw/deep_chal_multitask_dataset_test.parquet', '../data/image/test_images')\n",
    "# test = pd.read_parquet('../data/raw/deep_chal_multitask_dataset_test_path_converted.parquet')\n",
    "# test = test[test['input'].notna()]\n",
    "# test.to_parquet(\"../data/converted/deep_chal_multitask_dataset_test_path_converted.parquet\", index=False)\n",
    "\n",
    "# display(train.groupby(\"task\").head(1))\n",
    "# display(test.groupby(\"task\").head(1))\n",
    "# send_email(\"Preprocessing Done\", \"It is.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45453f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls ../data/image/train_images/ | wc -l\n",
    "# ! ls ../data/image/test_images/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2657848",
   "metadata": {},
   "source": [
    "# MS-Swift Custom Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bad3b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penultimate Train DF:  53213\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_path_converted.parquet\")\n",
    "\n",
    "text_qa_df = train[train[\"task\"] == \"text_qa\"].copy()\n",
    "all_qa_pairs = []\n",
    "for index, row in text_qa_df.iterrows():\n",
    "    context = row['input']\n",
    "    answers_dict = ast.literal_eval(row['output'])\n",
    "    questions_list = ast.literal_eval(row['question'])\n",
    "    answer_texts = answers_dict.get('input_text', [])\n",
    "    for question, answer in zip(questions_list, answer_texts):\n",
    "        all_qa_pairs.append({'input': context, 'question': question, 'output': answer})\n",
    "processed_qa_df = pd.DataFrame(all_qa_pairs)\n",
    "pronouns_to_remove = ['it', 'this', 'that', 'these', 'those', 'he', 'she', 'they', 'them', 'him', 'her', 'his', 'its', 'their', 'theirs']\n",
    "pattern = r'\\b(' + '|'.join(pronouns_to_remove) + r')\\b'\n",
    "mask_to_remove = processed_qa_df['question'].str.contains(pattern, case=False, regex=True)\n",
    "final_filtered_df = processed_qa_df[~mask_to_remove]\n",
    "allowed_start_words = ('What', 'How', 'Why', 'When', 'Where', 'Who', 'Is', 'Are', 'Am', 'Were', 'Was', 'Which', \"List\", \"Did\", \"Before\", \"After\", \"Name\", \"Do\", 'Can', 'Could', 'From', 'About', 'According', 'At', 'Can', 'Could', 'Define', 'Describe', 'Does', 'During', 'For', 'From', 'Had', 'Has', 'Have', 'In', 'On', 'Under', 'Whom', 'Whose', 'Will')\n",
    "mask_starts_with = final_filtered_df['question'].str.startswith(allowed_start_words)\n",
    "mask_long_enough = final_filtered_df['question'].str.split().str.len() >= 6\n",
    "final_mask = mask_starts_with & mask_long_enough\n",
    "final_filtered_df = final_filtered_df[final_mask]\n",
    "other_tasks_df = train[train['task'] != 'text_qa'].copy()\n",
    "final_filtered_df[\"input_type\"] = \"text\"\n",
    "final_filtered_df[\"task\"] = \"text_qa\"\n",
    "final_train_df = pd.concat([other_tasks_df, final_filtered_df], ignore_index=True)\n",
    "train = final_train_df.reindex()\n",
    "print(\"Penultimate Train DF: \", len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aa5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original task distribution:\n",
      "task\n",
      "text_qa           15860\n",
      "summarization     10000\n",
      "vqa               10000\n",
      "captioning         9880\n",
      "math_reasoning     7473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Task distribution after undersampling 'text_qa':\n",
      "task\n",
      "text_qa           11229\n",
      "summarization     10000\n",
      "vqa               10000\n",
      "captioning         9880\n",
      "math_reasoning     7473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Dataset Split ---\n",
      "Total samples: 48582\n",
      "SFT samples (90%): 43723\n",
      "\n",
      "--- Task Distribution in SFT set ---\n",
      "task\n",
      "text_qa           0.231137\n",
      "summarization     0.205841\n",
      "vqa               0.205841\n",
      "captioning        0.203371\n",
      "math_reasoning    0.153809\n",
      "Name: proportion, dtype: float64\n",
      "--------------------\n",
      "Saved ../data/dataset/sft_data_stratified.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32827</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41873</th>\n",
       "      <td>Given a math word problem, solve the question ...</td>\n",
       "      <td>There are 109 fifth graders + 115 sixth grader...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21671</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>₥</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>&lt;image&gt; Given a document image and a question,...</td>\n",
       "      <td>614-719-3245</td>\n",
       "      <td>[../data/image/train_images/b894a9acb477582814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31337</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15526</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>more</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>Given a math word problem, solve the question ...</td>\n",
       "      <td>If it takes 15 minutes to find each worm, then...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13984</th>\n",
       "      <td>&lt;image&gt; Given a document image and a question,...</td>\n",
       "      <td>1,451</td>\n",
       "      <td>[../data/image/train_images/61415755d8acbe5506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6184</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>&lt;image&gt; Generate a single, detailed, and objec...</td>\n",
       "      <td>\\nThe image is the cover of a science fiction ...</td>\n",
       "      <td>[../data/image/train_images/6366c4f474dea6c7a7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "32827  Given a context and a question, extract the mo...   \n",
       "41873  Given a math word problem, solve the question ...   \n",
       "21671  Given a context and a question, extract the mo...   \n",
       "31995  <image> Given a document image and a question,...   \n",
       "31337  Given a context and a question, extract the mo...   \n",
       "15526  Given a context and a question, extract the mo...   \n",
       "18183  Given a math word problem, solve the question ...   \n",
       "13984  <image> Given a document image and a question,...   \n",
       "6184   Given a context and a question, extract the mo...   \n",
       "34009  <image> Generate a single, detailed, and objec...   \n",
       "\n",
       "                                                  output  \\\n",
       "32827                                                yes   \n",
       "41873  There are 109 fifth graders + 115 sixth grader...   \n",
       "21671                                                  ₥   \n",
       "31995                                       614-719-3245   \n",
       "31337                                                CNN   \n",
       "15526                                               more   \n",
       "18183  If it takes 15 minutes to find each worm, then...   \n",
       "13984                                              1,451   \n",
       "6184                                             unknown   \n",
       "34009  \\nThe image is the cover of a science fiction ...   \n",
       "\n",
       "                                                  images  \n",
       "32827                                                NaN  \n",
       "41873                                                NaN  \n",
       "21671                                                NaN  \n",
       "31995  [../data/image/train_images/b894a9acb477582814...  \n",
       "31337                                                NaN  \n",
       "15526                                                NaN  \n",
       "18183                                                NaN  \n",
       "13984  [../data/image/train_images/61415755d8acbe5506...  \n",
       "6184                                                 NaN  \n",
       "34009  [../data/image/train_images/6366c4f474dea6c7a7...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SFT_FILE = '../data/dataset/sft_data_stratified.jsonl'\n",
    "IMAGE_PATH = '../data/image/'\n",
    "# VAL_FILE = '../data/dataset/val_data_stratified.jsonl'\n",
    "SPLIT_RATIO = 0.1\n",
    "\n",
    "df = train.copy()\n",
    "df['question'] = df['question'].fillna('')\n",
    "df['input'] = df['input'].fillna('')\n",
    "print(\"Original task distribution:\")\n",
    "print(df['task'].value_counts())\n",
    "\n",
    "df_text_qa = df[df['task'] == 'text_qa']\n",
    "df_other_tasks = df[df['task'] != 'text_qa']\n",
    "df_text_qa_undersampled = df_text_qa.groupby('input', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 3), random_state=42))\n",
    "df = pd.concat([df_other_tasks, df_text_qa_undersampled])\n",
    "print(\"\\nTask distribution after undersampling 'text_qa':\")\n",
    "print(df['task'].value_counts())\n",
    "\n",
    "sft_df, val_df = train_test_split(df, test_size=SPLIT_RATIO, random_state=42, stratify=df['task'])\n",
    "\n",
    "print(\"\\n--- Dataset Split ---\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"SFT samples (90%): {len(sft_df)}\")\n",
    "\n",
    "print(\"\\n--- Task Distribution in SFT set ---\")\n",
    "print(sft_df['task'].value_counts(normalize=True))\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "with open(SFT_FILE, 'w') as f:\n",
    "    for _, row in sft_df.iterrows():\n",
    "        task = row['task']\n",
    "        question = row['question']\n",
    "        output = row['output']\n",
    "        record = {}\n",
    "        if task == 'captioning':\n",
    "            query = '<image> Generate a single, detailed, and objective descriptive paragraph for the given image. Each description must begin with the phrase \"The image is...\" or \"The image shows...\", followed by a structured analysis that moves from the main subject to its details, and then to the background elements. You must use positional language, such as \"on the left\" or \"at the top of the cover\" to clearly orient the reader. If any text is visible in the image, transcribe it exactly and describe its visual characteristics like color and style. Conclude the entire description with a sentence that summarizes the overall atmosphere of the image, using a phrase like \"The overall mood of the image is...\". Throughout the paragraph, maintain a strictly factual, declarative tone with specific, descriptive vocabulary, avoiding any personal opinions or interpretations.'\n",
    "            image_path = os.path.join(IMAGE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"output\": output, \"images\": [image_path]}\n",
    "        elif task == 'vqa':\n",
    "            query = f'<image> Given a document image and a question, extract the precise answer. Your response must be only the literal text found in the image, with no extra words or explanation.\\n\\nQuestion: {question}'\n",
    "            image_path = os.path.join(IMAGE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"output\": output, \"images\": [image_path]}\n",
    "        elif task == 'summarization':\n",
    "            prompt = f\"Generate a summary of the following legislative text. Start with the bill's official title, then state its primary purpose and key provisions. Use formal, objective language and focus on the actions the bill takes, such as what it amends, requires, prohibits, or establishes.\\n\\nText: {row['input']}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        elif task == 'text_qa':\n",
    "            prompt = f\"Given a context and a question, extract the most concise, direct answer from the text. Your answer should be a short phrase, not a complete sentence.\\n\\nContext: {row['input']}\\n\\nQuestion: {question}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        elif task == 'math_reasoning':\n",
    "            prompt = f\"Given a math word problem, solve the question by generating a step-by-step reasoning process. After detailing all the steps in your reasoning, you must conclude your response by placing the final numerical answer on its own separate line, prefixed with #### .\\n\\nQuestion: {row['input']}\"\n",
    "            record = {\"input\": prompt, \"output\": output}\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "print(f\"Saved {SFT_FILE}\")\n",
    "display(pd.read_json(SFT_FILE, lines=True).sample(10))\n",
    "\n",
    "# NO VALIDATION FOR FASTER REPRODUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ffa417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../data/dataset/test_data.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>&lt;image&gt; Given a document image and a question,...</td>\n",
       "      <td>[../data/image/test_images/26d3e764cc55545b7af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>Generate a summary of the following legislativ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>Generate a summary of the following legislativ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;image&gt; Generate a single, detailed, and objec...</td>\n",
       "      <td>[../data/image/test_images/a81904a8d0ffc7181d6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>Given a context and a question, extract the mo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>Generate a summary of the following legislativ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>&lt;image&gt; Generate a single, detailed, and objec...</td>\n",
       "      <td>[../data/image/test_images/ed014d3b58228adbe10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>&lt;image&gt; Given a document image and a question,...</td>\n",
       "      <td>[../data/image/test_images/77e82c0248b1a087db5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "2024  Given a context and a question, extract the mo...   \n",
       "567   <image> Given a document image and a question,...   \n",
       "1811  Generate a summary of the following legislativ...   \n",
       "1824  Generate a summary of the following legislativ...   \n",
       "16    <image> Generate a single, detailed, and objec...   \n",
       "2092  Given a context and a question, extract the mo...   \n",
       "2085  Given a context and a question, extract the mo...   \n",
       "1903  Generate a summary of the following legislativ...   \n",
       "59    <image> Generate a single, detailed, and objec...   \n",
       "527   <image> Given a document image and a question,...   \n",
       "\n",
       "                                                 images  \n",
       "2024                                                NaN  \n",
       "567   [../data/image/test_images/26d3e764cc55545b7af...  \n",
       "1811                                                NaN  \n",
       "1824                                                NaN  \n",
       "16    [../data/image/test_images/a81904a8d0ffc7181d6...  \n",
       "2092                                                NaN  \n",
       "2085                                                NaN  \n",
       "1903                                                NaN  \n",
       "59    [../data/image/test_images/ed014d3b58228adbe10...  \n",
       "527   [../data/image/test_images/77e82c0248b1a087db5...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_parquet(\"../data/converted/deep_chal_multitask_dataset_test_path_converted.parquet\")\n",
    "TEST_FILE = '../data/dataset/test_data.jsonl'\n",
    "IMAGE_PATH = '../data/image/'\n",
    "test_df = test.copy()\n",
    "\n",
    "with open(TEST_FILE, 'w') as f:\n",
    "    for _, row in test_df.iterrows():\n",
    "        task = row['task']\n",
    "        question = row['question']\n",
    "        record = {}\n",
    "        if task == 'captioning':\n",
    "            query = '<image> Generate a single, detailed, and objective descriptive paragraph for the given image. Each description must begin with the phrase \"The image is...\" or \"The image shows...\", followed by a structured analysis that moves from the main subject to its details, and then to the background elements. You must use positional language, such as \"on the left\" or \"at the top of the cover\" to clearly orient the reader. If any text is visible in the image, transcribe it exactly and describe its visual characteristics like color and style. Conclude the entire description with a sentence that summarizes the overall atmosphere of the image, using a phrase like \"The overall mood of the image is...\". Throughout the paragraph, maintain a strictly factual, declarative tone with specific, descriptive vocabulary, avoiding any personal opinions or interpretations.'\n",
    "            image_path = os.path.join(IMAGE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"images\": [image_path]}\n",
    "        elif task == 'vqa':\n",
    "            query = f'<image> Given a document image and a question, extract the precise answer. Your response must be only the literal text found in the image, with no extra words or explanation.\\n\\nQuestion: {question}'\n",
    "            image_path = os.path.join(IMAGE_PATH, row['input'])\n",
    "            record = {\"input\": query, \"images\": [image_path]}\n",
    "        elif task == 'summarization':\n",
    "            prompt = f\"Generate a summary of the following legislative text. Start with the bill's official title, then state its primary purpose and key provisions. Use formal, objective language and focus on the actions the bill takes, such as what it amends, requires, prohibits, or establishes.\\n\\nText: {row['input']}\"\n",
    "            record = {\"input\": prompt}\n",
    "        elif task == 'text_qa':\n",
    "            prompt = f\"Given a context and a question, extract the most concise, direct answer from the text. Your answer should be a short phrase, not a complete sentence.\\n\\nContext: {row['input']}\\n\\nQuestion: {question}\"\n",
    "            record = {\"input\": prompt}\n",
    "        elif task == 'math_reasoning':\n",
    "            prompt = f\"Given a math word problem, solve the question by generating a step-by-step reasoning process. After detailing all the steps in your reasoning, you must conclude your response by placing the final numerical answer on its own separate line, prefixed with #### .\\n\\nQuestion: {row['input']}\"\n",
    "            record = {\"input\": prompt}\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "print(f\"Saved {TEST_FILE}\")\n",
    "\n",
    "display(pd.read_json(TEST_FILE, lines=True).sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2af214",
   "metadata": {},
   "source": [
    "# SFT w/ CE Loss, Resume w/ DFT Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b1d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model AIDC-AI/Ovis2.5-9B \\\n",
    "    --dataset '../data/dataset/sft_data_stratified.jsonl' \\\n",
    "    --dataloader_num_workers 16 \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --logging_steps 1 \\\n",
    "    --output_dir 'output_ce_loss' \\\n",
    "    --use_hf true \\\n",
    "    --train_type lora \\\n",
    "    --lora_rank 64 \\\n",
    "    --lora_alpha 128 \\\n",
    "    --init_weights pissa \\\n",
    "    --use_rslora true \\\n",
    "    --target_modules all-linear \\\n",
    "    --freeze_vit true \\\n",
    "    --freeze_llm false \\\n",
    "    --freeze_aligner true \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --max_length 8192 \\\n",
    "    --loss_scale ignore_empty_think \\\n",
    "    --attn_impl flash_attn \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --padding_free true \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --weight_decay 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db889eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES MS-SWIFT==3.8.0.dev0, WHICH SHOULD BE BUILT FROM (STABLE) SOURCE DIRECTLY\n",
    "# REPLACE CHECKPOINT PATH WITH 2600TH SFT CHECKPOINT\n",
    "\n",
    "# ! CUDA_VISIBLE_DEVICES=0 \\\n",
    "# swift sft \\\n",
    "#     --model AIDC-AI/Ovis2.5-9B \\\n",
    "#     --dataset '../data/dataset/sft_data_stratified.jsonl' \\\n",
    "#     --resume_from_checkpoint 'output_ce_loss/v0-20250828-230749/checkpoint-2600/'\n",
    "#     --dataloader_num_workers 16 \\\n",
    "#     --save_steps 200 \\\n",
    "#     --save_total_limit 5 \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --output_dir 'output_dft_loss' \\\n",
    "#     --use_hf true \\\n",
    "#     --train_type lora \\\n",
    "#     --lora_rank 64 \\\n",
    "#     --lora_alpha 128 \\\n",
    "#     --init_weights pissa \\\n",
    "#     --use_rslora true \\\n",
    "#     --target_modules all-linear \\\n",
    "#     --freeze_vit true \\\n",
    "#     --freeze_llm false \\\n",
    "#     --freeze_aligner true \\\n",
    "#     --torch_dtype bfloat16 \\\n",
    "#     --max_length 8192 \\\n",
    "#     --enable_dft_loss true \\\n",
    "#     --loss_scale ignore_empty_think \\\n",
    "#     --attn_impl flash_attn \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --per_device_train_batch_size 8 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --gradient_accumulation_steps 2 \\\n",
    "#     --padding_free true \\\n",
    "#     --warmup_ratio 0.1 \\\n",
    "#     --weight_decay 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"SFT Done\", \"WAKE UP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de063d3d",
   "metadata": {},
   "source": [
    "# Inference, Read Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c470990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# **** CHANGE THE ADAPTERS PATH INTO CORRECT, LAST CHECKPOINT PATH TO LOAD PROPERLY ****\n",
    "# **** AFTER SUBMISSION FILE CREATION, SUBMIT AND USE SUBMISSION WITH HIGHER SCORING ON LB ****\n",
    "# **** PLAY WITH TEMPERATURE, USED TEMP=[0, 0.15, 0.3] ****\n",
    "# **** CURRENT ADAPTER PATH IS SET TO THE ADAPTER I USED IN THE COMPETITION ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift infer \\\n",
    "    --adapters 'output_dft_loss/v0-20250828-230749/checkpoint-2733/' \\\n",
    "    --infer_backend pt \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 4096 \\\n",
    "    --val_dataset '../data/dataset/test_data.jsonl' \\\n",
    "    --use_hf true \\\n",
    "    --max_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** THE PREDICTION PATH IS AT THE END OF LAST CELL OUTPUT ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"../data/raw/\")\n",
    "pred = pd.read_json(\"output_dft_loss/v0-20250828-230749/checkpoint-2733/infer_result/20250830-115559.jsonl\", lines=True)\n",
    "test[\"output\"] = pred[\"response\"]\n",
    "test.index.name = \"id\"\n",
    "test[\"output\"].to_csv(\"../prediction/submission_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_email(\"Inference Done\", \"Check LB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52173966",
   "metadata": {},
   "source": [
    "# Simple ENV out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3163c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 freeze > ../requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
